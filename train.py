# -*- coding: utf-8 -*-
"""Untitled_Wiki.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16oBBSE4CxB7if-u1ZCrwBslljA-9Fkmb
"""

import torch
torch.cuda.empty_cache()
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, f1_score, recall_score,precision_score, accuracy_score,average_precision_score,precision_recall_curve,auc
import itertools
import networkx as nx
import torch.nn.functional as F
import numpy as np

import importlib
import hypergraph_construction
importlib.reload(hypergraph_construction)
import model_and_layers_modified
importlib.reload(model_and_layers_modified)
import data_preprocessing
importlib.reload(data_preprocessing)


from data_preprocessing import DataProcessor
from config import device
from hypergraph_construction import hyG1_function, hyG2_function, hyG3_function, LearnableMaskMatrix_for_knn_kmeans_based_hyG1,LearnableMaskMatrix_for_natural_hyG2,LearnableMaskMatrix_for_structure_based_hyG3
from model_and_layers_modified import THTN_attn1,Structure_aware_THTN_attn2,Structure_aware_THTN_attn3,centrality_values_function, GCN3
from loss import combined_contrastive_loss, combined_contrastive_loss_with_distance_based_neg_samples

dataset_name = 'pubmed'#[cora,citeseer, wiki, LastFMAsia, PT]
path_to_data = '/home/khaled/hcl/data'
path_to_label = f'{path_to_data}/label_{dataset_name}.txt'

# Load and prepare data
data_processor = DataProcessor(path_to_data, path_to_label, dataset_name)
g, G, com_DICT,coms_G, ego_DICT, egos_G,edge_index, default_feat, node_features, number_class= data_processor.get_data_components()
label=data_processor.label
train_label = data_processor.train_label
val_label = data_processor.val_label
test_label = data_processor.test_label


#HyG construction
num_hyperedges = G.number_of_nodes()
num_nodes = G.number_of_nodes()
knn_kmeans_mask_module = LearnableMaskMatrix_for_knn_kmeans_based_hyG1(num_hyperedges, num_nodes)
v_feat1 ,e_feat1, hyG1,LEN1,DICT_refined_knn_kmeans = hyG1_function(node_features,knn_kmeans_mask_module,G)


G_new=nx.Graph()
edges = zip(edge_index[0], edge_index[1])
G_new.add_edges_from(edges)

adj_dict = {n: list(G_new.adj[n]) for n in G_new.nodes()}
num_hyperedges=len(ego_DICT)
natural_hyG_mask_module = LearnableMaskMatrix_for_natural_hyG2(num_hyperedges, num_nodes)
v_feat2 ,e_feat2, hyG2,LEN2,egos_G_refined,DICT_refined_natural,uniqueness2 = hyG2_function(g,ego_DICT,egos_G,num_nodes,natural_hyG_mask_module)

num_hyperedges = len(com_DICT)
structural_hyG_mask_module = LearnableMaskMatrix_for_structure_based_hyG3(num_hyperedges, num_nodes)
v_feat3, e_feat3, hyG3,LEN3,coms_G_refined,DICT_refined,uniqueness3= hyG3_function(g,com_DICT,coms_G,num_nodes,structural_hyG_mask_module)


node_feat=default_feat
#gcn_model2=GCN2(node_feat.shape[1],64,LEN2)
gcn_model3=GCN3(node_feat.shape[1],512,LEN3)
g=g.add_self_loop()
centrality_values=centrality_values_function(G)

'''
Definition of all the models
'''
model1 =THTN_attn1(v_feat1.shape[1], 64, LEN1, LEN1,  number_class, 0.1,2) #semantical view
model2 =Structure_aware_THTN_attn2(v_feat2.shape[1], 64, LEN2, LEN2,  number_class,centrality_values,uniqueness2, 0.1,LEN2,2,G) #Natural view
model3 =Structure_aware_THTN_attn3(v_feat3.shape[1], 64, LEN3, LEN3,  number_class,centrality_values,uniqueness3,0.1,LEN3,2,G) #Structural view
models = [model1, model2, model3]
for model in models:
    model.to(device)

knn_kmeans_mask_module.capture_gradients()
natural_hyG_mask_module.capture_gradients()
structural_hyG_mask_module.capture_gradients()



'''
Pre-compute all shortest distane/paths lengths as a matrix
'''
shortest_paths_matrix = np.zeros((num_nodes, num_nodes))
for i, paths in nx.all_pairs_shortest_path_length(G):
    for j, length in paths.items():
        shortest_paths_matrix[i, j] = length
shortest_paths_tensor = torch.tensor(shortest_paths_matrix, dtype=torch.float32, device=device)


'''
Pre-compute all similarity scores as a matrix
'''
norm_node_features = F.normalize(node_feat, p=2, dim=1).to(device)
cosine_similarity_matrix = torch.mm(norm_node_features, norm_node_features.t())

########################################################################## Train-Test ########################################################################
kmeans_k=50
knn_k = 60
S = 2
E=0
best_val_acc = 0
patience=0
lambda_cl=0.2
lambda_l2 = 0.0001
epsilon =0.001
file_path = '/home/khaled/hcl/outputs/pubmed.txt'
tanh = nn.Tanh()

supervised_loss = nn.CrossEntropyLoss()
# Optimizer

optimizer = torch.optim.Adam(
    itertools.chain(
        model1.parameters(), 
        model2.parameters(),
        model3.parameters(),
        gcn_model3.parameters(),
        knn_kmeans_mask_module.parameters(),
        structural_hyG_mask_module.parameters(),
        natural_hyG_mask_module.parameters()
    ),
    weight_decay=1e-5,
    lr=0.001
)
with open(file_path, 'a') as file:
    for i in range(200):
        gcn_model2=0
        pred1,node_features1= model1(hyG1, v_feat1, e_feat1, True,True) 
        pred2,node_features2= model2(g, hyG2, centrality_values, uniqueness2,gcn_model2,node_feat,egos_G_refined, DICT_refined_natural,v_feat2, e_feat2, True,True)
        pred3,node_features3= model3(g, hyG3, centrality_values, uniqueness3,gcn_model3,node_feat,coms_G_refined, DICT_refined,v_feat3, e_feat3,                True,True)


        v_feat1 ,e_feat1, hyG1,LEN1,DICT_refined_knn_kmeans = hyG1_function(node_features1,knn_kmeans_mask_module,G) 
        v_feat2 ,e_feat2, hyG2,LEN2,egos_G_refined,DICT_refined_natural, uniqueness2 = hyG2_function(g,ego_DICT,egos_G,num_nodes,natural_hyG_mask_module)
        v_feat3, e_feat3, hyG3,LEN3,coms_G_refined,DICT_refined,uniqueness3 =   hyG3_function(g,com_DICT,coms_G,num_nodes,structural_hyG_mask_module)
        
        projected_node_features1 = node_features1
        projected_node_features2 = node_features2
        projected_node_features3 = node_features3
        

        contrastive_loss12 = combined_contrastive_loss_with_distance_based_neg_samples(projected_node_features1,projected_node_features2,  DICT_refined_knn_kmeans,DICT_refined_natural,edge_index,shortest_paths_tensor,cosine_similarity_matrix)

        contrastive_loss32 = combined_contrastive_loss_with_distance_based_neg_samples(projected_node_features3,projected_node_features2,   DICT_refined,DICT_refined_natural,edge_index,shortest_paths_tensor,cosine_similarity_matrix)

        contrastive_loss13 = combined_contrastive_loss_with_distance_based_neg_samples(projected_node_features1,projected_node_features3,     DICT_refined,DICT_refined_natural,edge_index,shortest_paths_tensor,cosine_similarity_matrix)


        total_contrastive_loss=(contrastive_loss12+contrastive_loss32+contrastive_loss13)/3

        pred = (pred1+ pred3)/2

        train_pred, test_pred= train_test_split( pred,test_size=0.8, random_state=7)
        size=int(len(label)*0.1)
        val_pred=train_pred[0:size]
        train_pred=train_pred[size:]

        loss = supervised_loss(train_pred, train_label)+lambda_cl*total_contrastive_loss

        
        # Add L2 regularization
        l2_reg = torch.tensor(0.).to(loss.device)
        param_count = 0
        for model in [model1, model2, model3,gcn_model3]:
            for param in model.parameters():
                l2_reg += torch.norm(param) ** 2
                param_count += param.numel()
        l2_reg = l2_reg / param_count  # Normalize by the total number of parameters
        loss = loss + lambda_l2 * l2_reg
        

        pred_cls = torch.argmax(train_pred, -1)
        train_acc = torch.eq(pred_cls, train_label).sum().item()/len(train_label)

        optimizer.zero_grad()
        loss.backward()
        # torch.nn.utils.clip_grad_norm_(itertools.chain(model1.parameters(), model2.parameters(), model3.parameters(), gcn_model.parameters(),  knn_kmeans_mask_module.parameters(), structural_hyG_mask_module.parameters()), max_norm=1.0)


        with torch.no_grad():
          knn_kmeans_mask_module.logits.data -= epsilon * knn_kmeans_mask_module.gradient_map
          natural_hyG_mask_module.logits.data -= epsilon * natural_hyG_mask_module.gradient_map
          structural_hyG_mask_module.logits.data -= epsilon * structural_hyG_mask_module.gradient_map

        optimizer.step()

        with torch.no_grad():
          val_cls = torch.argmax(val_pred, -1)
          val_acc = torch.eq(val_cls, val_label).sum().item()/len(val_label)

          if best_val_acc < val_acc:
            best_val_acc = val_acc
            E=i
            patience=0
            torch.save(test_pred,'/home/khaled/hcl/pth_folder/latest_pubmed.pth')
          else:
            patience+=1
          if patience==100:
            break
        if i % 10 == 0:
            train_statement='In epoch {}, train loss: {:.4f}, val_acc: {:.4f} (best_val_acc: {:.4f})'.format(i, loss, val_acc, best_val_acc)
            print(train_statement)
            file.write(train_statement+'\n')

    best_test_perd=torch.load('/home/khaled/hcl/pth_folder/latest_pubmed.pth')
    with torch.no_grad():
        test_pred=best_test_perd
        test_cls = torch.argmax(test_pred, -1)
        test_acc = torch.eq(test_cls, test_label).sum().item()/len(test_label)
        test_statement='(R25) GCN3, considering node1, and double org. loss, With Aug, Gradient Descent, WMLP, and Dis. based Sampled Neg (25): k_means: {}, k_nn: {}, S_value : {}, Best Epoch: {}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score {:.4f}'.format(kmeans_k,knn_k, S,E,accuracy_score(test_label.cpu(),test_cls.cpu()),precision_score(test_label.cpu(),test_cls.cpu(),average='weighted'),recall_score(test_label.cpu(),test_cls.        cpu(),average='weighted'),f1_score(test_label.cpu(),test_cls.cpu(),average='weighted'))
        print(test_statement)
        file.write(test_statement+'\n')
